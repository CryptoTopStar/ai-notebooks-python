{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a432ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the torch version works better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "1f1c594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "bd44db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset idea from https://github.com/karpathy/minGPT/blob/master/play_math.ipynb\n",
    "\n",
    "def make_dataset():\n",
    "  ret = []\n",
    "  for i in range(100):\n",
    "    for j in range(100):\n",
    "      s = i+j\n",
    "      ret.append([i//10, i%10, j//10, j%10, s//100, (s//10)%10, s%10])\n",
    "  return ret\n",
    "ds = make_dataset()\n",
    "random.shuffle(ds)\n",
    "ds = np.array(ds)\n",
    "ds_X = ds[:, 0:6]\n",
    "ds_Y = np.copy(ds[:, 1:])\n",
    "ds_X_train, ds_X_test = ds_X[0:8000], ds_X[8000:]\n",
    "ds_Y_train, ds_Y_test = ds_Y[0:8000], ds_Y[8000:]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(list(zip(ds_X_train, ds_Y_train)), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "71ff52e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(queries, keys, values):\n",
    "  d = queries.shape[-1]\n",
    "  scores = torch.matmul(queries, keys.transpose(-2,-1))/math.sqrt(d)\n",
    "  attention_weights = F.softmax(scores, dim=-1)\n",
    "  return torch.matmul(attention_weights, values)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, embed_dim, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.embed_dim, self.num_heads = embed_dim, num_heads\n",
    "    assert embed_dim % num_heads == 0\n",
    "    self.projection_dim = embed_dim // num_heads\n",
    "    \n",
    "    self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "    self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "    self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "    self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "  def transpose(self, x):\n",
    "    x = x.reshape(x.shape[0], x.shape[1], self.num_heads, self.projection_dim)\n",
    "    return x.permute(0, 2, 1, 3)\n",
    "  \n",
    "  def transpose_output(self, x):\n",
    "    x = x.permute(0, 2, 1, 3)\n",
    "    return x.reshape(x.shape[0], x.shape[1], self.embed_dim)\n",
    "    \n",
    "  def forward(self, q, k, v):\n",
    "    q = self.transpose(self.W_q(q))\n",
    "    k = self.transpose(self.W_k(k))\n",
    "    v = self.transpose(self.W_v(v))\n",
    "    output = attention(q, k, v)\n",
    "    return self.W_o(self.transpose_output(output))\n",
    "  \n",
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "    super(TransformerBlock, self).__init__()\n",
    "    self.att = MultiHeadAttention(embed_dim, num_heads)\n",
    "    self.ffn = nn.Sequential(\n",
    "      nn.Linear(embed_dim, ff_dim), nn.ReLU(), nn.Linear(ff_dim, embed_dim)\n",
    "    )\n",
    "    self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "    self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "    self.dropout = nn.Dropout(rate)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.layernorm1(x + self.dropout(self.att(x, x, x)))\n",
    "    x = self.layernorm2(x + self.dropout(self.ffn(x)))\n",
    "    return x\n",
    "  \n",
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "  def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "    super(TokenAndPositionEmbedding, self).__init__()\n",
    "    self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "    self.pos_emb = nn.Embedding(maxlen, embed_dim)\n",
    "  def forward(self, x):\n",
    "    pos = torch.arange(0, x.size(1), dtype=torch.int32)\n",
    "    return self.token_emb(x) + self.pos_emb(pos).view(1, x.size(1), -1)\n",
    "  \n",
    "m = nn.Sequential(\n",
    "  TokenAndPositionEmbedding(6, 10, 128),\n",
    "  TransformerBlock(128, 4, 32),\n",
    "  TransformerBlock(128, 4, 32),\n",
    "  nn.Linear(128, 10),\n",
    "  nn.LogSoftmax(dim=-1))\n",
    "opt = torch.optim.Adam(m.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "59d52020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:01<00:00, 135.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(446), tensor(10000)) tensor(176.6125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:01<00:00, 139.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(1801), tensor(10000)) tensor(62.5285)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:01<00:00, 139.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(2000), tensor(10000)) tensor(8.7012)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:01<00:00, 138.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(2000), tensor(10000)) tensor(1.9596)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:01<00:00, 138.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(2000), tensor(10000)) tensor(0.7682)\n"
     ]
    }
   ],
   "source": [
    "def num_correct():\n",
    "  m.eval()\n",
    "  pred = m(torch.from_numpy(ds_X_test)).argmax(dim=2)\n",
    "  gt = torch.from_numpy(ds_Y_test)\n",
    "  return (pred[:, -1] == gt[:, -1]).sum(), (pred[:, :-1] == gt[:, :-1]).sum()\n",
    "\n",
    "for epoch in range(5):\n",
    "  m.train()\n",
    "  total_loss = None\n",
    "  for dat in (t:=tqdm(train_loader)):\n",
    "    #print(dat[0].shape)\n",
    "    output = m(dat[0])\n",
    "    #print(output, dat[1])\n",
    "\n",
    "    loss = F.nll_loss(output.view(-1, 10), dat[1].view(-1))\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if total_loss == None:\n",
    "      total_loss = loss.detach()\n",
    "    else:\n",
    "      total_loss += loss.detach()\n",
    "    #t.set_description(\"%f\" % loss)\n",
    "  print(num_correct(), total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3497bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
